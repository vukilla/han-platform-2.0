diff --git a/hmr4d/dataset/bedlam/bedlam.py b/hmr4d/dataset/bedlam/bedlam.py
index 52b75ef..3c5a034 100644
--- a/hmr4d/dataset/bedlam/bedlam.py
+++ b/hmr4d/dataset/bedlam/bedlam.py
@@ -8,7 +8,10 @@ from time import time
 from hmr4d.configs import MainStore, builds
 from hmr4d.utils.smplx_utils import make_smplx
 from hmr4d.utils.wis3d_utils import make_wis3d, add_motion_as_lines
-from hmr4d.utils.vis.renderer_utils import simple_render_mesh_background
+try:
+    from hmr4d.utils.vis.renderer_utils import simple_render_mesh_background  # type: ignore
+except Exception:  # pragma: no cover
+    simple_render_mesh_background = None  # type: ignore
 from hmr4d.utils.video_io_utils import read_video_np, save_video
 
 import hmr4d.utils.matrix as matrix
diff --git a/hmr4d/dataset/h36m/h36m.py b/hmr4d/dataset/h36m/h36m.py
index 5370b0d..877bb36 100644
--- a/hmr4d/dataset/h36m/h36m.py
+++ b/hmr4d/dataset/h36m/h36m.py
@@ -14,7 +14,10 @@ from hmr4d.utils.geo_transform import compute_cam_angvel, apply_T_on_points
 from hmr4d.utils.geo.hmr_global import get_tgtcoord_rootparam, get_T_w2c_from_wcparams, get_c_rootparam, get_R_c2gv
 
 from hmr4d.utils.wis3d_utils import make_wis3d, add_motion_as_lines
-from hmr4d.utils.vis.renderer import Renderer
+try:
+    from hmr4d.utils.vis.renderer import Renderer  # type: ignore
+except Exception:  # pragma: no cover
+    Renderer = None  # type: ignore
 import imageio
 from hmr4d.utils.video_io_utils import read_video_np
 from hmr4d.utils.net_utils import get_valid_mask, repeat_to_max_len, repeat_to_max_len_dict
diff --git a/hmr4d/dataset/threedpw/threedpw_motion_train.py b/hmr4d/dataset/threedpw/threedpw_motion_train.py
index 2c803fd..c99c1ca 100644
--- a/hmr4d/dataset/threedpw/threedpw_motion_train.py
+++ b/hmr4d/dataset/threedpw/threedpw_motion_train.py
@@ -12,7 +12,10 @@ from hmr4d.dataset.imgfeat_motion.base_dataset import ImgfeatMotionDatasetBase
 from hmr4d.utils.net_utils import get_valid_mask, repeat_to_max_len, repeat_to_max_len_dict
 from hmr4d.utils.smplx_utils import make_smplx
 from hmr4d.utils.video_io_utils import get_video_lwh, read_video_np, save_video
-from hmr4d.utils.vis.renderer_utils import simple_render_mesh_background
+try:
+    from hmr4d.utils.vis.renderer_utils import simple_render_mesh_background  # type: ignore
+except Exception:  # pragma: no cover
+    simple_render_mesh_background = None  # type: ignore
 
 from hmr4d.configs import MainStore, builds
 
diff --git a/hmr4d/utils/body_model/body_model.py b/hmr4d/utils/body_model/body_model.py
index 5f8ac47..112524e 100644
--- a/hmr4d/utils/body_model/body_model.py
+++ b/hmr4d/utils/body_model/body_model.py
@@ -1,4 +1,3 @@
-from turtle import forward
 import numpy as np
 
 import torch
diff --git a/hmr4d/utils/preproc/__init__.py b/hmr4d/utils/preproc/__init__.py
index 6ef3897..55c80e8 100644
--- a/hmr4d/utils/preproc/__init__.py
+++ b/hmr4d/utils/preproc/__init__.py
@@ -1,7 +1,11 @@
 from hmr4d.utils.preproc.tracker import Tracker
 from hmr4d.utils.preproc.vitfeat_extractor import Extractor
 from hmr4d.utils.preproc.vitpose import VitPoseExtractor
-from hmr4d.utils.preproc.relpose.simple_vo import SimpleVO
+try:
+    # SimpleVO depends on pycolmap, which may not be available on Windows.
+    from hmr4d.utils.preproc.relpose.simple_vo import SimpleVO  # type: ignore
+except Exception:  # pragma: no cover
+    SimpleVO = None  # type: ignore
 
 try:
     from hmr4d.utils.preproc.slam import SLAMModel
diff --git a/hmr4d/utils/preproc/tracker.py b/hmr4d/utils/preproc/tracker.py
index b093ed4..2741e83 100644
--- a/hmr4d/utils/preproc/tracker.py
+++ b/hmr4d/utils/preproc/tracker.py
@@ -16,9 +16,41 @@ from hmr4d.utils.video_io_utils import get_video_lwh
 from hmr4d.utils.net_utils import moving_average_smooth
 
 
+def _allowlist_ultralytics_safe_globals() -> None:
+    # PyTorch 2.6+ defaults `torch.load(..., weights_only=True)` and uses a restricted
+    # unpickler. Ultralytics checkpoints reference their model classes, so we must
+    # allowlist them for safe loading on newer torch versions.
+    try:
+        # In practice, the Ultralytics YOLOv8 weights are a trusted checkpoint
+        # (downloaded from Ultralytics releases). To keep GVHMR working on torch>=2.6,
+        # force `weights_only=False` by default.
+        import inspect
+
+        if "weights_only" not in inspect.signature(torch.load).parameters:
+            return
+
+        if getattr(torch.load, "_han_ultralytics_patched", False):
+            return
+
+        _orig_torch_load = torch.load
+
+        def _torch_load_compat(*args, **kwargs):
+            # Force full load to avoid torch>=2.6 safe-unpickler failures on Ultralytics checkpoints.
+            kwargs["weights_only"] = False
+            return _orig_torch_load(*args, **kwargs)
+
+        _torch_load_compat._han_ultralytics_patched = True  # type: ignore[attr-defined]
+        torch.load = _torch_load_compat  # type: ignore[assignment]
+    except Exception:
+        # Best-effort: if the import path changes, Ultralytics may still work, or will
+        # fail with a clearer error prompting manual allowlisting.
+        return
+
+
 class Tracker:
     def __init__(self) -> None:
         # https://docs.ultralytics.com/modes/predict/
+        _allowlist_ultralytics_safe_globals()
         self.yolo = YOLO(PROJ_ROOT / "inputs/checkpoints/yolo/yolov8x.pt")
 
     def track(self, video_path):
@@ -30,14 +62,19 @@ class Tracker:
             "verbose": False,
             "stream": True,
         }
-        results = self.yolo.track(video_path, **cfg)
-        # frame-by-frame tracking
-        track_history = []
-        for result in tqdm(results, total=get_video_lwh(video_path)[0], desc="YoloV8 Tracking"):
-            if result.boxes.id is not None:
-                track_ids = result.boxes.id.int().cpu().tolist()  # (N)
+        # NOTE: Avoid Ultralytics `.track()` on Windows because it pulls in tracking
+        # dependencies (e.g. `lap/lapx`) and can attempt runtime `pip install`.
+        # For GVHMR preprocessing we only need a single-person bbox per frame, so
+        # `.predict()` is sufficient and much more robust.
+        results = self.yolo.predict(video_path, **cfg)
+        for result in tqdm(results, total=get_video_lwh(video_path)[0], desc="YoloV8 Detect"):
+            if result.boxes is not None and result.boxes.xyxy is not None and result.boxes.xyxy.shape[0] > 0:
                 bbx_xyxy = result.boxes.xyxy.cpu().numpy()  # (N, 4)
-                result_frame = [{"id": track_ids[i], "bbx_xyxy": bbx_xyxy[i]} for i in range(len(track_ids))]
+                best_idx = 0
+                if getattr(result.boxes, "conf", None) is not None and result.boxes.conf.shape[0] == bbx_xyxy.shape[0]:
+                    best_idx = int(result.boxes.conf.argmax().item())
+                # Single tracked ID (person=0) for downstream code.
+                result_frame = [{"id": 0, "bbx_xyxy": bbx_xyxy[best_idx]}]
             else:
                 result_frame = []
             track_history.append(result_frame)
diff --git a/hmr4d/utils/vis/renderer.py b/hmr4d/utils/vis/renderer.py
index f4d6232..0f47a5a 100644
--- a/hmr4d/utils/vis/renderer.py
+++ b/hmr4d/utils/vis/renderer.py
@@ -2,20 +2,51 @@ import cv2
 import torch
 import numpy as np
 
-from pytorch3d.renderer import (
-    PerspectiveCameras,
-    TexturesVertex,
-    PointLights,
-    Materials,
-    RasterizationSettings,
-    MeshRenderer,
-    MeshRasterizer,
-    SoftPhongShader,
-)
-from pytorch3d.structures import Meshes
-from pytorch3d.structures.meshes import join_meshes_as_scene
-from pytorch3d.renderer.cameras import look_at_rotation
-from pytorch3d.transforms import axis_angle_to_matrix
+_HAS_PYTORCH3D_RENDERER = True
+_PYTORCH3D_IMPORT_ERROR = None
+
+try:
+    from pytorch3d.renderer import (
+        PerspectiveCameras,
+        TexturesVertex,
+        PointLights,
+        Materials,
+        RasterizationSettings,
+        MeshRenderer,
+        MeshRasterizer,
+        SoftPhongShader,
+    )
+    from pytorch3d.structures import Meshes
+    from pytorch3d.structures.meshes import join_meshes_as_scene
+    from pytorch3d.renderer.cameras import look_at_rotation
+    from pytorch3d.transforms import axis_angle_to_matrix
+except Exception as exc:  # noqa: BLE001
+    # GVHMR's renderer is optional for our pipeline. We run pose extraction only and
+    # pass `--skip_render` to the demo. To keep imports working on Windows where
+    # installing full pytorch3d is painful, we allow this module to import without it.
+    _HAS_PYTORCH3D_RENDERER = False
+    _PYTORCH3D_IMPORT_ERROR = exc
+    PerspectiveCameras = None  # type: ignore[assignment]
+    TexturesVertex = None  # type: ignore[assignment]
+    PointLights = None  # type: ignore[assignment]
+    Materials = None  # type: ignore[assignment]
+    RasterizationSettings = None  # type: ignore[assignment]
+    MeshRenderer = None  # type: ignore[assignment]
+    MeshRasterizer = None  # type: ignore[assignment]
+    SoftPhongShader = None  # type: ignore[assignment]
+    Meshes = None  # type: ignore[assignment]
+    join_meshes_as_scene = None  # type: ignore[assignment]
+    look_at_rotation = None  # type: ignore[assignment]
+    axis_angle_to_matrix = None  # type: ignore[assignment]
+
+
+def _require_pytorch3d() -> None:
+    if not _HAS_PYTORCH3D_RENDERER:
+        raise ImportError(
+            "GVHMR rendering requires `pytorch3d` (renderer/structures). "
+            "Install pytorch3d or run with `--skip_render`. "
+            f"Original error: {_PYTORCH3D_IMPORT_ERROR}"
+        )
 
 from .renderer_tools import get_colors, checkerboard_geometry
 
@@ -105,6 +136,7 @@ def compute_bbox_from_points(X, img_w, img_h, scaleFactor=1.2):
 class Renderer:
     def __init__(self, width, height, focal_length=None, device="cuda", faces=None, K=None, bin_size=None):
         """set bin_size to 0 for no binning"""
+        _require_pytorch3d()
         self.width = width
         self.height = height
         self.bin_size = bin_size
@@ -276,6 +308,7 @@ class Renderer:
 
 
 def create_meshes(verts, faces, colors):
+    _require_pytorch3d()
     """
     :param verts (B, V, 3)
     :param faces (B, F, 3)
@@ -287,6 +320,7 @@ def create_meshes(verts, faces, colors):
 
 
 def get_global_cameras(verts, device="cuda", distance=5, position=(-5.0, 5.0, 0.0)):
+    _require_pytorch3d()
     """This always put object at the center of view"""
     positions = torch.tensor([position]).repeat(len(verts), 1)
     targets = verts.mean(1)
@@ -305,6 +339,7 @@ def get_global_cameras(verts, device="cuda", distance=5, position=(-5.0, 5.0, 0.
 def get_global_cameras_static(
     verts, beta=4.0, cam_height_degree=30, target_center_height=1.0, use_long_axis=False, vec_rot=45, device="cuda"
 ):
+    _require_pytorch3d()
     L, V, _ = verts.shape
 
     # Compute target trajectory, denote as center + scale
diff --git a/hmr4d/utils/wis3d_utils.py b/hmr4d/utils/wis3d_utils.py
index df54d8b..78caa92 100644
--- a/hmr4d/utils/wis3d_utils.py
+++ b/hmr4d/utils/wis3d_utils.py
@@ -1,4 +1,7 @@
-from wis3d import Wis3D
+try:
+    from wis3d import Wis3D  # type: ignore
+except Exception:  # pragma: no cover
+    Wis3D = None  # type: ignore
 from pathlib import Path
 from datetime import datetime
 import torch
@@ -19,6 +22,10 @@ def make_wis3d(output_dir="outputs/wis3d", name="debug", time_postfix=False):
         time_str = datetime.now().strftime("%m%d-%H%M-%S")
         name = f"{name}_{time_str}"
         print(f"Creating Wis3D {name}")
+    if Wis3D is None:
+        raise ImportError(
+            "wis3d is not installed. Install it or avoid calling make_wis3d() in headless/Windows runs."
+        )
     wis3d = Wis3D(output_dir.absolute(), name)
     return wis3d
 
diff --git a/tools/demo/demo.py b/tools/demo/demo.py
index a10fb64..1f8965f 100644
--- a/tools/demo/demo.py
+++ b/tools/demo/demo.py
@@ -3,6 +3,7 @@ import torch
 import pytorch_lightning as pl
 import numpy as np
 import argparse
+import os
 from hmr4d.utils.pylogger import Log
 import hydra
 from hydra import initialize_config_module, compose
@@ -27,7 +28,6 @@ from hmr4d.utils.geo_transform import compute_cam_angvel
 from hmr4d.model.gvhmr.gvhmr_pl_demo import DemoPL
 from hmr4d.utils.net_utils import detach_to_cpu, to_cuda
 from hmr4d.utils.smplx_utils import make_smplx
-from hmr4d.utils.vis.renderer import Renderer, get_global_cameras_static, get_ground_params_from_points
 from tqdm import tqdm
 from hmr4d.utils.geo_transform import apply_T_on_points, compute_T_ayfz2ay
 from einops import einsum, rearrange
@@ -36,6 +36,22 @@ from einops import einsum, rearrange
 CRF = 23  # 17 is lossless, every +6 halves the mp4 size
 
 
+def _get_render_device() -> str:
+    dev = str(os.environ.get("GVHMR_RENDER_DEVICE", "cuda") or "cuda").strip().lower()
+    if dev not in ("cuda", "cpu"):
+        dev = "cuda"
+    if dev == "cuda" and not torch.cuda.is_available():
+        dev = "cpu"
+    return dev
+
+
+def _should_render_incam() -> bool:
+    # Our platform UI already shows the original video on the left, so the "incam"
+    # overlay video is often unnecessary. Allow skipping it via env var.
+    val = str(os.environ.get("GVHMR_RENDER_INCAM", "1") or "1").strip().lower()
+    return val in ("1", "true", "yes")
+
+
 def parse_args_to_cfg():
     # Put all args to cfg
     parser = argparse.ArgumentParser()
@@ -52,6 +68,11 @@ def parse_args_to_cfg():
         "If the camera zoom in a lot, you can try 135, 200 or even larger values.",
     )
     parser.add_argument("--verbose", action="store_true", help="If true, draw intermediate results")
+    parser.add_argument(
+        "--skip_render",
+        action="store_true",
+        help="Skip rendering videos. Useful for headless inference and Windows environments without PyTorch3D renderer.",
+    )
     args = parser.parse_args()
 
     # Input
@@ -92,7 +113,7 @@ def parse_args_to_cfg():
         writer.close()
         reader.close()
 
-    return cfg
+    return cfg, args
 
 
 @torch.no_grad()
@@ -200,20 +221,25 @@ def load_data_dict(cfg):
     return data
 
 
-def render_incam(cfg):
+def render_incam(cfg, render_device=None):
+    # Import renderer lazily so environments without pytorch3d.renderer can still run inference.
+    from hmr4d.utils.vis.renderer import Renderer
+
+    if render_device is None:
+        render_device = _get_render_device()
+
     incam_video_path = Path(cfg.paths.incam_video)
     if incam_video_path.exists():
         Log.info(f"[Render Incam] Video already exists at {incam_video_path}")
         return
 
     pred = torch.load(cfg.paths.hmr4d_results)
-    smplx = make_smplx("supermotion").cuda()
-    smplx2smpl = torch.load("hmr4d/utils/body_model/smplx2smpl_sparse.pt").cuda()
-    faces_smpl = make_smplx("smpl").faces
+    smplx = make_smplx("supermotion").to(render_device)
+    faces_smplx = smplx.faces
 
-    # smpl
-    smplx_out = smplx(**to_cuda(pred["smpl_params_incam"]))
-    pred_c_verts = torch.stack([torch.matmul(smplx2smpl, v_) for v_ in smplx_out.vertices])
+    # smplx
+    smplx_out = smplx(**(to_cuda(pred["smpl_params_incam"]) if render_device == "cuda" else pred["smpl_params_incam"]))
+    pred_c_verts = smplx_out.vertices
 
     # -- rendering code -- #
     video_path = cfg.video_path
@@ -221,15 +247,33 @@ def render_incam(cfg):
     K = pred["K_fullimg"][0]
 
     # renderer
-    renderer = Renderer(width, height, device="cuda", faces=faces_smpl, K=K)
+    verts_incam = pred_c_verts
+    if render_device == "cpu":
+        verts_incam = verts_incam.detach().cpu()
+    renderer = Renderer(width, height, device=render_device, faces=faces_smplx, K=K)
+
+    # Preflight: Blackwell GPUs (e.g. RTX 5090) can fail with prebuilt PyTorch3D wheels.
+    # Fall back to CPU rendering so we still produce a mesh preview.
+    try:
+        dummy_bg = np.ones((height, width, 3), dtype=np.uint8) * 255
+        _ = renderer.render_mesh(verts_incam[0].to(render_device), dummy_bg, [0.8, 0.8, 0.8])
+    except RuntimeError as exc:
+        msg = str(exc).lower()
+        if render_device == "cuda" and "no kernel image" in msg:
+            Log.info("[Render Incam] CUDA rasterizer unsupported; falling back to CPU.")
+            render_device = "cpu"
+            verts_incam = pred_c_verts.detach().cpu()
+            renderer = Renderer(width, height, device=render_device, faces=faces_smplx, K=K)
+        else:
+            raise
+
     reader = get_video_reader(video_path)  # (F, H, W, 3), uint8, numpy
     bbx_xys_render = torch.load(cfg.paths.bbx)["bbx_xys"]
 
     # -- render mesh -- #
-    verts_incam = pred_c_verts
     writer = get_writer(incam_video_path, fps=30, crf=CRF)
     for i, img_raw in tqdm(enumerate(reader), total=get_video_lwh(video_path)[0], desc=f"Rendering Incam"):
-        img = renderer.render_mesh(verts_incam[i].cuda(), img_raw, [0.8, 0.8, 0.8])
+        img = renderer.render_mesh(verts_incam[i].to(render_device), img_raw, [0.8, 0.8, 0.8])
 
         # # bbx
         # bbx_xys_ = bbx_xys_render[i].cpu().numpy()
@@ -242,7 +286,13 @@ def render_incam(cfg):
     reader.close()
 
 
-def render_global(cfg):
+def render_global(cfg, render_device=None):
+    # Import renderer lazily so environments without pytorch3d.renderer can still run inference.
+    from hmr4d.utils.vis.renderer import Renderer, get_global_cameras_static, get_ground_params_from_points
+
+    if render_device is None:
+        render_device = _get_render_device()
+
     global_video_path = Path(cfg.paths.global_video)
     if global_video_path.exists():
         Log.info(f"[Render Global] Video already exists at {global_video_path}")
@@ -250,34 +300,49 @@ def render_global(cfg):
 
     debug_cam = False
     pred = torch.load(cfg.paths.hmr4d_results)
-    smplx = make_smplx("supermotion").cuda()
-    smplx2smpl = torch.load("hmr4d/utils/body_model/smplx2smpl_sparse.pt").cuda()
-    faces_smpl = make_smplx("smpl").faces
-    J_regressor = torch.load("hmr4d/utils/body_model/smpl_neutral_J_regressor.pt").cuda()
+    smplx = make_smplx("supermotion").to(render_device)
+    faces_smplx = smplx.faces
+
+    from hmr4d.utils.body_model.smplx_lite import SmplxLiteSmplN24
+
+    smpl24 = SmplxLiteSmplN24().to(render_device)
 
-    # smpl
-    smplx_out = smplx(**to_cuda(pred["smpl_params_global"]))
-    pred_ay_verts = torch.stack([torch.matmul(smplx2smpl, v_) for v_ in smplx_out.vertices])
+    params = to_cuda(pred["smpl_params_global"]) if render_device == "cuda" else pred["smpl_params_global"]
 
-    def move_to_start_point_face_z(verts):
+    # smplx vertices + SMPL-24 joints (for stable "face-z" transform and ground plane sizing).
+    smplx_out = smplx(**params)
+    verts_glob = smplx_out.vertices  # (L, V, 3)
+    joints_glob = smpl24(
+        params["body_pose"],
+        params["betas"],
+        params["global_orient"],
+        params.get("transl", None),
+    )  # (L, 24, 3)
+
+    def move_to_start_point_face_z(verts: torch.Tensor, joints: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
         "XZ to origin, Start from the ground, Face-Z"
-        # position
-        verts = verts.clone()  # (L, V, 3)
-        offset = einsum(J_regressor, verts[0], "j v, v i -> j i")[0]  # (3)
-        offset[1] = verts[:, :, [1]].min()
+        verts = verts.clone()
+        joints = joints.clone()
+
+        min_y = verts[..., 1].min()
+        offset = joints[0, 0].detach().clone()
+        offset[1] = min_y
         verts = verts - offset
-        # face direction
-        T_ay2ayfz = compute_T_ayfz2ay(einsum(J_regressor, verts[[0]], "j v, l v i -> l j i"), inverse=True)
-        verts = apply_T_on_points(verts, T_ay2ayfz)
-        return verts
+        joints = joints - offset
+
+        T_ay2ayfz = compute_T_ayfz2ay(joints[[0]], inverse=True)  # (1, 4, 4)
+        T_seq = T_ay2ayfz.repeat(joints.shape[0], 1, 1)
+        verts = apply_T_on_points(verts, T_seq)
+        joints = apply_T_on_points(joints, T_seq)
+        return verts, joints
 
-    verts_glob = move_to_start_point_face_z(pred_ay_verts)
-    joints_glob = einsum(J_regressor, verts_glob, "j v, l v i -> l j i")  # (L, J, 3)
+    verts_glob, joints_glob = move_to_start_point_face_z(verts_glob, joints_glob)
     global_R, global_T, global_lights = get_global_cameras_static(
         verts_glob.cpu(),
         beta=2.0,
         cam_height_degree=20,
         target_center_height=1.0,
+        device=render_device,
     )
 
     # -- rendering code -- #
@@ -286,25 +351,55 @@ def render_global(cfg):
     _, _, K = create_camera_sensor(width, height, 24)  # render as 24mm lens
 
     # renderer
-    renderer = Renderer(width, height, device="cuda", faces=faces_smpl, K=K)
+    verts_glob_render = verts_glob
+    joints_glob_render = joints_glob
+    if render_device == "cpu":
+        verts_glob_render = verts_glob_render.detach().cpu()
+        joints_glob_render = joints_glob_render.detach().cpu()
+    renderer = Renderer(width, height, device=render_device, faces=faces_smplx, K=K)
     # renderer = Renderer(width, height, device="cuda", faces=faces_smpl, K=K, bin_size=0)
 
     # -- render mesh -- #
-    scale, cx, cz = get_ground_params_from_points(joints_glob[:, 0], verts_glob)
+    scale, cx, cz = get_ground_params_from_points(joints_glob_render[:, 0], verts_glob_render)
     renderer.set_ground(scale * 1.5, cx, cz)
-    color = torch.ones(3).float().cuda() * 0.8
+    color = torch.ones(3).float().to(render_device) * 0.8
+
+    # Preflight: if CUDA kernels are missing for this GPU, fall back to CPU rendering.
+    try:
+        cameras0 = renderer.create_camera(global_R[0], global_T[0])
+        _ = renderer.render_with_ground(verts_glob_render[[0]], color[None], cameras0, global_lights)
+    except RuntimeError as exc:
+        msg = str(exc).lower()
+        if render_device == "cuda" and "no kernel image" in msg:
+            Log.info("[Render Global] CUDA rasterizer unsupported; falling back to CPU.")
+            render_device = "cpu"
+            global_R, global_T, global_lights = get_global_cameras_static(
+                verts_glob.cpu(),
+                beta=2.0,
+                cam_height_degree=20,
+                target_center_height=1.0,
+                device=render_device,
+            )
+            verts_glob_render = verts_glob.detach().cpu()
+            joints_glob_render = joints_glob.detach().cpu()
+            renderer = Renderer(width, height, device=render_device, faces=faces_smplx, K=K)
+            scale, cx, cz = get_ground_params_from_points(joints_glob_render[:, 0], verts_glob_render)
+            renderer.set_ground(scale * 1.5, cx, cz)
+            color = torch.ones(3).float().to(render_device) * 0.8
+        else:
+            raise
 
     render_length = length if not debug_cam else 8
     writer = get_writer(global_video_path, fps=30, crf=CRF)
     for i in tqdm(range(render_length), desc=f"Rendering Global"):
         cameras = renderer.create_camera(global_R[i], global_T[i])
-        img = renderer.render_with_ground(verts_glob[[i]], color[None], cameras, global_lights)
+        img = renderer.render_with_ground(verts_glob_render[[i]], color[None], cameras, global_lights)
         writer.write_frame(img)
     writer.close()
 
 
 if __name__ == "__main__":
-    cfg = parse_args_to_cfg()
+    cfg, args = parse_args_to_cfg()
     paths = cfg.paths
     Log.info(f"[GPU]: {torch.cuda.get_device_name()}")
     Log.info(f'[GPU]: {torch.cuda.get_device_properties("cuda")}')
@@ -327,8 +422,15 @@ if __name__ == "__main__":
         torch.save(pred, paths.hmr4d_results)
 
     # ===== Render ===== #
-    render_incam(cfg)
-    render_global(cfg)
-    if not Path(paths.incam_global_horiz_video).exists():
-        Log.info("[Merge Videos]")
-        merge_videos_horizontal([paths.incam_video, paths.global_video], paths.incam_global_horiz_video)
+    if args.skip_render:
+        Log.info("[Render] Skipping rendering (--skip_render).")
+    else:
+        render_device = _get_render_device()
+        render_incam_enabled = _should_render_incam()
+        Log.info(f"[Render Device]: {render_device}")
+        if render_incam_enabled:
+            render_incam(cfg, render_device=render_device)
+        render_global(cfg, render_device=render_device)
+        if render_incam_enabled and not Path(paths.incam_global_horiz_video).exists():
+            Log.info("[Merge Videos]")
+            merge_videos_horizontal([paths.incam_video, paths.global_video], paths.incam_global_horiz_video)
